{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8ff781e6-0c0f-45da-a6ac-cbdc74b3765b",
   "metadata": {},
   "source": [
    "## High-Level ETL (Extract - Transform - Load) Flow\n",
    "**Goal**: By the end of this tutorial, you will be able to\n",
    "- Extract: Download a file from AWS S3 using Pythonâ€™s boto3.\n",
    "- Transform: Clean, filter, or manipulate data in Python (often using libraries like pandas).\n",
    "- Load: Insert the transformed data into a relational database via SQL statements."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24d4c364-5538-4205-bc71-4a9e631eeefc",
   "metadata": {},
   "source": [
    "## Lab Assignment\n",
    "\n",
    "1. Implement the following functions\n",
    "   - `extract_from_csv(file_to_process: str) -> pd.DataFrame`: read the .csv file and return dataframe\n",
    "   - `extract_from_json(file_to_process: str) -> pd.DataFrame`: read the .json file and return dataframe\n",
    "   - `extract() -> pd.DataFrame`: extract data of heterogeneous format and combine them into a single dataframe.\n",
    "   - `transform(df) -> pd.DataFrame`: function for data cleaning and manipulation.\n",
    "2. Clean the data\n",
    "   - Round float-type columns to two decimal places.\n",
    "   - remove duplicate samples\n",
    "   - Save the cleaned data into parquet file\n",
    "3. Insert the data into SQL\n",
    "   - Create postgresql database\n",
    "   - Insert the data into the database\n",
    "  \n",
    "Submission requirement:\n",
    "    1. Jupyter Notebook\n",
    "    2. Parquet File\n",
    "    3. SQL file (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "a9594048-5e1f-4138-b202-30eabce92d06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting psycopg2\n",
      "  Using cached psycopg2-2.9.10.tar.gz (385 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hBuilding wheels for collected packages: psycopg2\n",
      "  Building wheel for psycopg2 (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for psycopg2: filename=psycopg2-2.9.10-cp311-cp311-macosx_11_0_arm64.whl size=132267 sha256=ef5459840ea76a0d9500c347258100ba9eb049d6780728e5ea1c50d573090676\n",
      "  Stored in directory: /Users/annamcgillis/Library/Caches/pip/wheels/d9/83/60/e9660320860aef3c38a67dea6ff9538e4cad76502cb39ed280\n",
      "Successfully built psycopg2\n",
      "Installing collected packages: psycopg2\n",
      "Successfully installed psycopg2-2.9.10\n",
      "Requirement already satisfied: sqlalchemy in /opt/anaconda3/lib/python3.11/site-packages (2.0.25)\n",
      "Requirement already satisfied: typing-extensions>=4.6.0 in /opt/anaconda3/lib/python3.11/site-packages (from sqlalchemy) (4.9.0)\n",
      "Requirement already satisfied: pyarrow in /opt/anaconda3/lib/python3.11/site-packages (14.0.2)\n",
      "Requirement already satisfied: numpy>=1.16.6 in /opt/anaconda3/lib/python3.11/site-packages (from pyarrow) (1.26.4)\n"
     ]
    }
   ],
   "source": [
    "# Required Package:\n",
    "!pip install psycopg2\n",
    "#pip install pandas 2.0.3 (For data manipulation and analysis)\n",
    "!pip install sqlalchemy\n",
    "!pip install pyarrow\n",
    "import pandas as pd\n",
    "\n",
    "#required for reading .xml files\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "#required for navigating machine's directory\n",
    "import glob\n",
    "import os.path\n",
    "\n",
    "#required for communicating with SQL database\n",
    "from sqlalchemy import create_engine"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b26ea5ad-4aa4-4d75-8cca-41de1e7454f3",
   "metadata": {},
   "source": [
    "# E: Extracting data from multiple sources"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14f9a43c-2bc6-4de5-9356-f36c88d95a86",
   "metadata": {},
   "source": [
    "This code was removed to allow push to github (secret information)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b14027a8-8a6d-4566-93ba-710e68b97f3e",
   "metadata": {},
   "source": [
    "## Extract data from ./data/ folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "03b2bdcb-fb78-419c-a3e9-9f12ee8f2b65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./data/used_car_prices1.csv\n",
      "./data/used_car_prices2.csv\n",
      "./data/used_car_prices3.csv\n",
      "./data/used_car_prices1.json\n",
      "./data/used_car_prices3.xml\n",
      "./data/used_car_prices2.xml\n",
      "./data/used_car_prices3.json\n",
      "./data/used_car_prices1.xml\n",
      "./data/used_car_prices2.json\n"
     ]
    }
   ],
   "source": [
    "all_files = glob.glob('./data/*')\n",
    "\n",
    "# Output the list of files\n",
    "for file in all_files:\n",
    "    print(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c30b5a8-7206-4a8c-8837-b42ab76115fb",
   "metadata": {},
   "source": [
    "### Function to extract data from one .csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3ac06a7b-ea5d-4410-b0b3-be9db100d2bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_from_csv(file_to_process: str) -> pd.DataFrame:\n",
    "    dataframe = pd.read_csv(file_to_process)\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7051e6b2-6f3a-4b2c-9ff4-1e10a91a296e",
   "metadata": {},
   "source": [
    "### Function to extract data from one .json file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "31b19053-3d8a-4386-82d9-5a7b287d37db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_from_json(file_to_process: str) -> pd.DataFrame:\n",
    "    dataframe = pd.read_json(file_to_process, lines=True)\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "072dfa48-6805-451e-9d0f-eb09de70e9b1",
   "metadata": {},
   "source": [
    "### Function to extract data from one  .xml file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5f6bd897-d626-43f3-9980-e3c1e0e27ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_from_xml(file_to_process: str) -> pd.DataFrame:\n",
    "    dataframe = pd.DataFrame(columns = columns)\n",
    "    tree = ET.parse(file_to_process)\n",
    "    root = tree.getroot()\n",
    "    for person in root:\n",
    "        car_model = person.find(\"car_model\").text\n",
    "        year_of_manufacture = int(person.find(\"year_of_manufacture\").text)\n",
    "        price = float(person.find(\"price\").text)\n",
    "        fuel = person.find(\"fuel\").text\n",
    "        sample = pd.DataFrame({\"car_model\":car_model, \"year_of_manufacture\":year_of_manufacture, \"price\":price, \"fuel\":fuel}, index = [0])\n",
    "        dataframe = pd.concat([dataframe, sample], ignore_index=True)\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1e2e693-0d74-4e9e-92e1-395119aab10e",
   "metadata": {},
   "source": [
    "### Function to extract data from the ./data/ folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7fbc266d-1ef2-49bb-8784-4a8ccbffd038",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract() -> pd.DataFrame:\n",
    "    extracted_data = pd.DataFrame(columns = columns)\n",
    "    #for csv files\n",
    "    for csv_file in glob.glob(os.path.join(folder, \"*.csv\")):\n",
    "        extracted_data = pd.concat([extracted_data, extract_from_csv(csv_file)], ignore_index=True)\n",
    "    \n",
    "    #add lines for json files\n",
    "    for json_file in glob.glob(os.path.join(folder, \"*.json\")):\n",
    "        extracted_data = pd.concat([extracted_data, extract_from_json(json_file)], ignore_index=True)\n",
    "    \n",
    "    #add lines for xml files\n",
    "    for xml_file in glob.glob(os.path.join(folder, \"*.xml\")):\n",
    "        extracted_data = pd.concat([extracted_data, extract_from_xml(xml_file)], ignore_index=True)\n",
    "    \n",
    "    return extracted_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81cafa18-245e-4e2c-a63b-6854eb93b863",
   "metadata": {},
   "source": [
    "### Extract the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "6a70a8ce-348f-4b26-8a9a-3ead42565c74",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/wd/g730h79n6g90zsm1jp44dyv00000gn/T/ipykernel_26257/1959904625.py:5: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  extracted_data = pd.concat([extracted_data, extract_from_csv(csv_file)], ignore_index=True)\n",
      "/var/folders/wd/g730h79n6g90zsm1jp44dyv00000gn/T/ipykernel_26257/3387287639.py:11: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  dataframe = pd.concat([dataframe, sample], ignore_index=True)\n",
      "/var/folders/wd/g730h79n6g90zsm1jp44dyv00000gn/T/ipykernel_26257/3387287639.py:11: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  dataframe = pd.concat([dataframe, sample], ignore_index=True)\n",
      "/var/folders/wd/g730h79n6g90zsm1jp44dyv00000gn/T/ipykernel_26257/3387287639.py:11: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  dataframe = pd.concat([dataframe, sample], ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "columns = ['car_model','year_of_manufacture','price', 'fuel']\n",
    "folder = \"data\"\n",
    "#table_name = \"car_data\"\n",
    "\n",
    "# run\n",
    "def main():\n",
    "    data = extract()\n",
    "    #insert_to_table(data, \"car_data\")\n",
    "    \n",
    "    return data\n",
    "\n",
    "data = main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "5b1288a8-86fb-498c-833a-682ecd9c1eb6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>car_model</th>\n",
       "      <th>year_of_manufacture</th>\n",
       "      <th>price</th>\n",
       "      <th>fuel</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ritz</td>\n",
       "      <td>2014</td>\n",
       "      <td>5000.000000</td>\n",
       "      <td>Petrol</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sx4</td>\n",
       "      <td>2013</td>\n",
       "      <td>7089.552239</td>\n",
       "      <td>Diesel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ciaz</td>\n",
       "      <td>2017</td>\n",
       "      <td>10820.895522</td>\n",
       "      <td>Petrol</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>wagon r</td>\n",
       "      <td>2011</td>\n",
       "      <td>4253.731343</td>\n",
       "      <td>Petrol</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>swift</td>\n",
       "      <td>2014</td>\n",
       "      <td>6865.671642</td>\n",
       "      <td>Diesel</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  car_model year_of_manufacture         price    fuel\n",
       "0      ritz                2014   5000.000000  Petrol\n",
       "1       sx4                2013   7089.552239  Diesel\n",
       "2      ciaz                2017  10820.895522  Petrol\n",
       "3   wagon r                2011   4253.731343  Petrol\n",
       "4     swift                2014   6865.671642  Diesel"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5265c408-8c63-413f-ad04-8d391e5b564d",
   "metadata": {},
   "source": [
    "# T: Transformation data and save organized data to .parquet file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "faae9e13-ded5-47e0-8316-13ec6084ed56",
   "metadata": {},
   "outputs": [],
   "source": [
    "staging_file = \"cars.parquet\"\n",
    "staging_data_dir = \"staging_data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "eefdc72e-6359-4363-a648-86082888f2a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform(df):\n",
    "    print(f\"Shape of data {df.shape}\")\n",
    "\n",
    "    # truncate price with 2 decimal place (add your code below)\n",
    "    df['price'] = df['price'].apply(lambda x: round(x, 2))\n",
    "\n",
    "    # remove samples with same car_model (add your code below)\n",
    "    df = df.drop_duplicates(subset=['car_model'])\n",
    "    \n",
    "    print(f\"Shape of data {df.shape}\")\n",
    "\n",
    "    # Ensure the staging directory exists before writing the Parquet file\n",
    "    if not os.path.exists(staging_data_dir):\n",
    "        os.makedirs(staging_data_dir)\n",
    "        print(f\"Directory '{staging_data_dir}' created.\")\n",
    "\n",
    "    # write to parquet\n",
    "    df.to_parquet(os.path.join(staging_data_dir, staging_file))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "4f5d5423-c198-4aa7-89f7-354992f97324",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data (90, 4)\n",
      "Shape of data (25, 4)\n",
      "Directory 'staging_data' created.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>car_model</th>\n",
       "      <th>year_of_manufacture</th>\n",
       "      <th>price</th>\n",
       "      <th>fuel</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ritz</td>\n",
       "      <td>2014</td>\n",
       "      <td>5000.00</td>\n",
       "      <td>Petrol</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sx4</td>\n",
       "      <td>2013</td>\n",
       "      <td>7089.55</td>\n",
       "      <td>Diesel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ciaz</td>\n",
       "      <td>2017</td>\n",
       "      <td>10820.90</td>\n",
       "      <td>Petrol</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>wagon r</td>\n",
       "      <td>2011</td>\n",
       "      <td>4253.73</td>\n",
       "      <td>Petrol</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>swift</td>\n",
       "      <td>2014</td>\n",
       "      <td>6865.67</td>\n",
       "      <td>Diesel</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  car_model year_of_manufacture     price    fuel\n",
       "0      ritz                2014   5000.00  Petrol\n",
       "1       sx4                2013   7089.55  Diesel\n",
       "2      ciaz                2017  10820.90  Petrol\n",
       "3   wagon r                2011   4253.73  Petrol\n",
       "4     swift                2014   6865.67  Diesel"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print the head of your data\n",
    "df = transform(data)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a960d531-a731-42d4-906c-4602391a16ca",
   "metadata": {},
   "source": [
    "# L: Loading data for further modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bef01f8d-a8b6-454c-a842-61c44cf04efc",
   "metadata": {},
   "source": [
    "### Set Up PostgreSQL Locally\n",
    "#### Step 1: Install PostgreSQL\n",
    "- Windows: Download from MySQL Official Site {https://www.postgresql.org/download/}\n",
    "- Mac:\n",
    "  ```{bash}\n",
    "  brew install postgresql\n",
    "  brew services start postgresql\n",
    "  ```\n",
    "Then access PostgreSQL CLI\n",
    "```{bash}\n",
    "psql -U postgres\n",
    "```\n",
    "Note: if you don't have default \"postgres\" user, then create it manually by\n",
    "```{bash}\n",
    "default \"postgres\" user\n",
    "```\n",
    "or\n",
    "```{bash}\n",
    "sudo -u $(whoami) createuser postgres -s\n",
    "```\n",
    "\n",
    "Then create a database\n",
    "```{sql}\n",
    "CREATE DATABASE my_local_db;\n",
    "\\l  -- List all databases\n",
    "```\n",
    "\n",
    "#### Step 2: Create a User and Grant Privileges\n",
    "In PostgreSQL CLI:\n",
    "```{sql}\n",
    "CREATE USER myuser WITH ENCRYPTED PASSWORD 'mypassword';\n",
    "GRANT ALL PRIVILEGES ON DATABASE my_local_db TO myuser;\n",
    "```\n",
    "\n",
    "#### Step 3: Install Required Python Libraries\n",
    "```{bash}\n",
    "pip install pandas sqlalchemy pymysql psycopg2 mysql-connector-python\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b270a29e-c73a-4ca1-b7e8-64ec621cb29a",
   "metadata": {},
   "source": [
    "### Utility function for writing data into the SQL database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "6f188de2-ae27-444c-8590-4c1dde9ac7e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Database credentials\n",
    "db_host = \"localhost\"\n",
    "db_user = \"myuser\"\n",
    "db_password = \"mypassword\"\n",
    "db_name = \"my_local_db\"\n",
    "\n",
    "conn_string = f\"postgresql+psycopg2://{db_user}:{db_password}@{db_host}/{db_name}\"\n",
    "\n",
    "engine = create_engine(conn_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "7cdb749a-4467-4264-b23b-f0693ee980af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            schemaname                tablename    tableowner tablespace  \\\n",
      "0           pg_catalog             pg_statistic  annamcgillis       None   \n",
      "1           pg_catalog                  pg_type  annamcgillis       None   \n",
      "2           pg_catalog         pg_foreign_table  annamcgillis       None   \n",
      "3           pg_catalog                pg_authid  annamcgillis  pg_global   \n",
      "4           pg_catalog    pg_statistic_ext_data  annamcgillis       None   \n",
      "..                 ...                      ...           ...        ...   \n",
      "61          pg_catalog           pg_largeobject  annamcgillis       None   \n",
      "62  information_schema                sql_parts  annamcgillis       None   \n",
      "63  information_schema  sql_implementation_info  annamcgillis       None   \n",
      "64  information_schema             sql_features  annamcgillis       None   \n",
      "65  information_schema               sql_sizing  annamcgillis       None   \n",
      "\n",
      "    hasindexes  hasrules  hastriggers  rowsecurity  \n",
      "0         True     False        False        False  \n",
      "1         True     False        False        False  \n",
      "2         True     False        False        False  \n",
      "3         True     False        False        False  \n",
      "4         True     False        False        False  \n",
      "..         ...       ...          ...          ...  \n",
      "61        True     False        False        False  \n",
      "62       False     False        False        False  \n",
      "63       False     False        False        False  \n",
      "64       False     False        False        False  \n",
      "65       False     False        False        False  \n",
      "\n",
      "[66 rows x 8 columns]\n"
     ]
    }
   ],
   "source": [
    "# Test connection\n",
    "df = pd.read_sql(\"SELECT * FROM pg_catalog.pg_tables;\", con=engine)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "f2fed596-5b37-4da6-927d-facc292e733e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_to_table(data: pd.DataFrame, conn_string:str, table_name:str):\n",
    "    db = create_engine(conn_string) # creates a connection to the database using SQLAlchemy\n",
    "    conn = db.connect() # Establishes a database connection\n",
    "    data.to_sql(table_name, conn, if_exists=\"replace\", index=False)\n",
    "    conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "db20ac0d-c828-45a1-b29a-a8cf20e5cdf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25, 4)\n"
     ]
    }
   ],
   "source": [
    "# read from the .parquet file\n",
    "\n",
    "def load() -> pd.DataFrame:\n",
    "    data = pd.DataFrame()\n",
    "    for parquet_file in glob.glob(os.path.join(staging_data_dir, \"*.parquet\")):\n",
    "        data = pd.concat([pd.read_parquet(parquet_file),data])\n",
    "\n",
    "    #insert_to_table(data, table_name)\n",
    "    insert_to_table(data = data, conn_string = conn_string, table_name = 'ml_car_data')\n",
    "\n",
    "    return data\n",
    "\n",
    "data = load()\n",
    "print(data.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
